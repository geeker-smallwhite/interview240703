### 1. 注意力机制

注意力机制是源自人脑处理外部信息的机制，举个例子，人脑每时每刻都接收大量的外部信息，信息的数量远超过人脑的处理能力，所以人在处理信息的时候，会将注意力放到需要关注的信息上，忽略无用信息，这就是注意力机制。

#### 1.1 自注意力

注意力中的提示分为非自主提示和自主提示

- 非自主提示指的是信息本身具有突出的特征能直接被提取出来；
- 自主提示指的是在以往的经验的介入下可以将数据的特征信息提取出来。

`Transformer` 中的自注意力机制是通过 `(Q、K、V)` 来实现的。

- `Q` 即 `Query`， 指的是自主提示，是从经验中查询出来的特征向量，即主观意识的特征向量。
- `K` 即 `Key` ， 指的是非自主提示，是物体本身就存在的突出特征向量，即客观存在的特征向量。
- `V` 即 `Value`，指的是物体本身的向量。

> `attention is all you need` 论文中并没有提及如何提取 `Q、K` 特征向量，但本质上都是原向量经过一个神经网络得到的，我们将其简化为，`V` 与 一个权重向量的乘积，输入 `V` 也是经过处理的，所以 `Q、K、V`可以这样来表示。
> $$
> \begin{array}{l}
> Q=X W^{Q} \\
> K=X W^{K} \\
> V=X W^{V} \\
> \end{array}
> $$
> 其中 X 表示输入向量，`W` 分别代表 `Q、K、V` 的权重向量。

自注意力机制就是通过 `Query` 和 `Key`  注意力汇聚实现对 `Value` 的注意力权重分配，生成最终的输出结果。

`transformer` 框架是通过 **缩放点积注意力** 对 `Q、K、V` 向量进行计算。

<img src="https://typra-pictures.oss-cn-beijing.aliyuncs.com/imgs/image-20240707120003179.png" alt="image-20240707120003179" style="zoom: 50%;" />

1. 将 `Q` 和 `K` 进行汇聚，$K^T$ 是 `K` 矩阵的转置。

	> - `𝑄`： 形状为 $(n_{q},d_{k})$，其中 $n_q$  是查询向量的数量（通常与输入序列长度相同），$d_k$ 是键/查询向量的维度。
	> - `K`： 形状为 $(n_{k},d_{k})$，其中 $n_k$  是键向量的数量（通常与输入序列长度相同），$d_k$ 是键/查询向量的维度。
	>
	> 两向量无法直接点乘，需要转置。

2. 根据 `K` 的维度 $d_{k}$，将汇聚结果进行缩放。

3. 然后通过 `Softmax` 激活函数判断 `V` 是否可取。

$$
\operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$

#### 1.2 多头注意力

在实际场景中，对一个物品、一句话可以能存在多个特征，使用单一特征去进行拟合是不合理的，所以需要多头注意力来捕捉多种特征。

> 每个注意力头采用独立的 `Q、K、V` ，可以独立捕捉单独的特征。

<img src="https://typra-pictures.oss-cn-beijing.aliyuncs.com/imgs/image-20240707122246194.png" alt="image-20240707122246194" style="zoom:50%;" />

依据图中内容，将 `h` 个自注意力层中得到的结果直接拼接到一起。
$$
\begin{array}{l}
\operatorname{MultiHead}(Q, K, V)=\operatorname{Concat}\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\
\text { where head }{ }_{\mathrm{i}}=\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \\
\end{array}
$$
将拼接的结果发送给一个神经网络，得到结果，这就是一个多头注意力子层。



### 2. 前馈神经网络

除了 `multi-attention` 子层之外，编码器和解码器中的每一层都包含一个全连接的前馈网络`(FFN，Feed-Forward Neural Network)`。
$$
FFN(x)=max(0, xW_1+b_1)W_2 + b_2
$$

1. 非线性变换与特征提取

	前馈神经网络包含两个线性变换和激活函数（通常是 `ReLU`），这使得模型能够进行复杂的非线性变换，从而提取和表示输入数据的更复杂特征。自注意力机制主要用于捕捉序列中的关系和上下文依赖性，但前馈神经网络进一步对这些捕捉到的特征进行非线性变换，增强模型的特征提取能力。

2. 增加模型复杂性和表达能力

	`FFN` 增加了模型的参数量和复杂性，使得模型能够表示更复杂的映射函数。虽然自注意力机制已经非常强大，但通过将其与FFN结合，模型能够在捕捉序列关系的基础上，进行更深层次的特征抽象和处理。

3. 局部特征处理

	前馈神经网络能够对每个位置的特征单独进行处理，这与自注意力机制在序列全局范围内计算特征得分的方式互补。在处理每个位置时，前馈神经网络不会考虑其他位置的信息，因此，这部分操作是独立的，这有利于并行计算优化。



### 3. 编码层和解码层

编码/解码层主要由多头自注意力子层和的前馈神经网络子层组成，每个子层后面还跟着 **残差连接和层归一化**。

> **残差连接**是指将输入直接与输出相加，将部分信息直接传递到下一层，从而避免了信息的丢失。这种连接方式可以使网络更容易训练，并且有助于提高模型的性能。 
>
> **层归一化**是一种归一化技术，在每一层的输入中对特征进行归一化，能够加速网络的收敛速度，提高模型的泛化能力。相较于传统的批归一化，层归一化更适用于处理变长输入，因此在 Transformer 模型中得到了广泛的应用。

<img src="https://typra-pictures.oss-cn-beijing.aliyuncs.com/imgs/image-20240707214236378.png" alt="image-20240707214236378" style="zoom: 67%;" />



### 4. `Transformer` 结构

`Transformer` 架构，编码器和解码器均使用堆叠的自注意力和逐点全连接层。

1.  **位置编码 (Positional Encoding)**

	位置编码 `(Position Encoding)` 通过把位置信息引入输入序列中，以打破模型的全对称性。

2. **编码层 (Encode) **

	编码器由 `N = 6` 个相同的层组成。每一层有两个子层。第一个是多头自注意力机制，第二个是简单的、按位置全连接的前馈网络。我们采用残差连接围绕每两个子层，然后层归一化。也就是说，每个子层的输出是 `LayerNorm(x + Sublayer(x))`。

3. **解码层 (Decode)**

	解码器也由 `N = 6` 个相同层的堆栈组成。除了每个编码器层中的两个子层外，解码器插入第三个子层，掩码自注意力机制`(Masked Multi-Head Attention)`，该子层对编码器堆栈的输出执行多头注意力。与编码器类似，我们在每个子层周围采用残差连接，然后进行层归一化。

	> `Transformer` 中的掩码多头注意力 `(Masked Multi-Head Attention)` 主要作用是为了**防止信息泄漏，保证输出信息的因果论。**在生成任务中，比如文本生成，预测不能依赖未来信息。掩码机制通过遮蔽未来的输入，确保了模型在预测当前词时只能看见历史信息和当前步的信息，而不能看未来的信息。这种机制保证了模型的生成过程是按序进行的。
	>
	> 具体而言，掩码多头注意力的工作流程如下： 
	>
	> 1. **掩码操作：**   在计算注意力得分时，通过掩码操作，为那些我们不希望关注的位置（如未来的时间步）赋予一个非常小的值（如负无穷），这样在经过 `Softmax `函数归一化后，这些位置的注意力权重几乎为零。
	> 2.  **计算多头注意力：**   然后，对于每个头，计算注意力权重并加权求和，以得到每个位置的表示。这些表示再被拼接起来，通过一个线性变换，得到最终的输出。 这种掩码机制的核心作用就是在确保生成任务中模型只使用合法的信息来进行预测，同时保持计算效率和模型的表达能力。

<img src="https://typra-pictures.oss-cn-beijing.aliyuncs.com/imgs/image-20240707213202996.png" alt="image-20240707213202996" style="zoom:50%;" />



---

### 5. `Transformer`  的应用

`Transformer` 的多头注意力在实际应用场景中主要有三种实现方式：

- 标准 `Transformer`，该结构就是 `attention is all you need` 论文中提到的，由编码器`(Encoder)`和解码器`(Decoder)`组成。

	- 编码器：由多层相同结构的编码器层组成，每层包括多头自注意力机制和前馈神经网络。
	- 解码器：由多层相同结构的解码器层组成，除了自注意力机制和前馈神经网络，还包括一个用于获取编码器输出的编码器-解码器注意力层。

	该结构的模型主要用于机器翻译等序列到序列 `(seq2seq)` 任务。

- 仅编码器`(BERT：Bidirectional Encoder Representations from Transformers)`：该结构是由`Google `提出，专注于仅使用 `Transformer` 的编码器部分，该架构只有编码器，没有解码器。

	- `BERT` 通过双向编码器的方式对文本进行预训练，使其能够捕捉到词语的前后文信息。

	`BERT` 模型在各种 `NLP` 任务（如问答系统、文本分类、命名实体识别等）中有广泛应用，主要用于理解任务（如分类、问答）。

- 仅解码器 `(GPT, Generative Pre-training Transformer)`，该结构是由 `OpenAI` 提出，专注于使用 `Transformer` 的解码器部分。

	- `GPT` 通过自回归的方法生成文本，顺序地预测下一个词。

	`GPT`在生成任务（如文本生成、对话系统等）中表现优异，主要用于生成任务（如文本生成）